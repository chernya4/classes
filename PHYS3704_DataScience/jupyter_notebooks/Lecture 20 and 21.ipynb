{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "%matplotlib inline   \n",
    "                     # this sets up matplotlib to make plots show up in the notebook\n",
    "import numpy as np   # imports the numpy package, abbreviated as np\n",
    "import matplotlib    # imports the matplotlib package for making plots\n",
    "import matplotlib.pyplot as plt    # imports the part of matplotlib we use most,\n",
    " \n",
    "import numpy.random as random\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing some things from before . . .\n",
    "\n",
    "Set up two sets of 18 values from normal distributions, N(0,1) or N(1,1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the arrays\n",
    "ndata=18\n",
    "data1=random.randn(ndata)\n",
    "data2=random.randn(ndata)+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find mean of each array\n",
    "mean1=np.mean(data1)\n",
    "mean2=np.mean(data2)\n",
    "\n",
    "sigma1=np.std(data1,ddof=1)/np.sqrt(ndata) # want the standard deviation of the mean of data1\n",
    "sigma2=np.std(data2,ddof=1)/np.sqrt(ndata) # want the standard deviation of the mean of data2\n",
    "\n",
    "\n",
    "print(f'means: {mean1:.4f} , {mean2:.4f}')\n",
    "print(f'sigmas: {sigma1:.4f} , {sigma2:.4f}')\n",
    "\n",
    "tfactor=stats.t.ppf(1-0.025,ndata-1)\n",
    "print(f'Confidence Interval for mean 1: [ {mean1-tfactor*sigma1:.4f} , {mean1+tfactor*sigma1:.4f} ]')\n",
    "print(f'Confidence Interval for mean 2: [ {mean2-tfactor*sigma2:.4f} , {mean2+tfactor*sigma2:.4f} ]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence intervals for the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff=mean2-mean1\n",
    "sigma_diff=np.sqrt(sigma1**2 + sigma2**2)\n",
    "tfactor=stats.t.ppf(1-0.025, 2*ndata-2)\n",
    "\n",
    "print(f'Observed difference of means: {mean_diff:.4f} ')\n",
    "\n",
    "print(f'2-sided Confidence Interval: [ {mean_diff-tfactor*sigma_diff:.4f} , {mean_diff+tfactor*sigma_diff:.4f} ]')\n",
    "\n",
    "tfactor=stats.t.ppf(1-0.05, 2*ndata-2)\n",
    "print(f'1-sided Confidence Interval: > {mean_diff-tfactor*sigma_diff:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation tests\n",
    "\n",
    "We will combine `data1` and `data2` into 1 array; then generate sets of 2 datasets of size `ndata` and see how often their means differ as much as in the observed case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose number of bootstrap samples\n",
    "nsims=int(5E4)\n",
    "\n",
    "# make a combined dataset from both original data arrays\n",
    "datac=np.concatenate( (data1,data2) )\n",
    "\n",
    "# generate the two bootstrap samples\n",
    "fake1=np.random.choice(datac,size=(ndata,nsims) )\n",
    "fake2=np.random.choice(datac,size=(ndata,nsims) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the means for each simulated dataset: should be nsims elements in each array\n",
    "fakemeans1 = np.mean(fake1,axis=0)\n",
    "fakemeans2 = np.mean(fake2,axis=0)\n",
    "\n",
    "# Calculate the difference between the means\n",
    "diffs=fakemeans2-fakemeans1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using the below code box, plot histograms of the distributions of `fakemeans1` and `fakemeans2`, using the same binning and ~100 bins.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using the below code box, plot a histogram of the distribution of differences between the means of each sample(`diffs`), using ~100 bins.  Add a vertical dashed line at the observed value of the difference between the means of the two real data samples (`data1` and `data2`) (you can use `plt.axvline` for this).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print significance (alpha) = 0.32, 0.05, 0.01, and 0.001 limits on mean2-mean1\n",
    "print(f'cutoffs: {np.percentile(diffs,(68.,95.,99.,99.9))}')\n",
    "\n",
    "#make some empty space\n",
    "print()\n",
    "\n",
    "# compare the observed difference between the means to these cutoffs\n",
    "print(f'Observed difference of means: {mean_diff:.4f} ')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a p-value using `scipy.stats.percentileofscore(array,value)`, which returns the percentile in `array` corresponding to the observed value `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'p-value: {( 100-stats.percentileofscore(diffs,mean2-mean1) )/100.:.6g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to standard deviations\n",
    "\n",
    "We can also use the permutation test to investigate whether the ratio of standard deviations of the two samples could be different (note that each array by construction had the same intrinsic standard deviation, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate std. dev. of each bootstrap fake sample\n",
    "\n",
    "fakesigmas1=np.std(fake1,axis=0,ddof=1)\n",
    "fakesigmas2=np.std(fake2,axis=0,ddof=1)\n",
    "\n",
    "ratio=fakesigmas2/fakesigmas1\n",
    "realratio=np.std(data2,ddof=1)/np.std(data1,ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using the below code box, plot histograms of the two `fakesigmas` arrays, using ~100 bins each and the same binning.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using the below code box, plot a histogram of the ratio of the two sigmas, i.e. `ratio=fakesigma2/fakesigma1`, with ~100 bins.  Put a vertical line at the observed value of the ratio calculated from `data1` and `data2`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using the below code box, determine the limits of the 68 and 95 percent regions (NOT the 68/95th percentile points, as we want a 2-sided test) for the value of this ratio, determined using the permutation tests.__ \n",
    "\n",
    "__Compare these limits to the observed value.__\n",
    "\n",
    "__Also calculate the p-value for the observed ratio (again, keeping in mind that we want to do a 2-sided test, so we want to calculate the probability of anything either more extremely large OR more extremely small under the hypothesis that there is no difference): so this must be twice as large as the difference of your observed percentile/100  and either 0 or 1, whichever is closer...__\n",
    "\n",
    "__What do you conclude about whether the observed ratio of standard deviations is consistent with the hypothesis of no difference?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Kolmogorov-Smirnov Test\n",
    "\n",
    "Let's set up two sets of Poisson-distributed data, with mean 5 or 7.5, and see if we can detect differences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = 25\n",
    "countb = stats.poisson.rvs(5,size=(ndata) )\n",
    "countr =stats.poisson.rvs(7.5,size=(ndata) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Use the below code box to bring up the help on `stats.ks_2samp`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluate whether the K-S test below finds a statistically significant difference between the two samples.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d,p = stats.ks_2samp(countb,countr)\n",
    "print(f'd value: {d:.4f} , p-value: {p:.6g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Mann-Whitney U Test\n",
    "\n",
    "__Use the below code box to bring up the help on `stats.mannwhitneyu`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluate whether the U test below finds a statistically significant difference between the two samples.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,p = stats.mannwhitneyu(countb,countr)\n",
    "print(f'U value: {u:.4f} , p-value: {p:.4g}')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
